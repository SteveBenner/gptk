Bundler.require :default, :ai

module GPTK
  # AI interfaces and tools
  # TODO: rewrite the interface so that cache and memory calls are self contained within GPTK::AI
  module AI
    @last_output = nil # Track the cached output of the latest operation

    # OpenAI's ChatGPT interface
    module ChatGPT
      # Sends a query to an AI client using a simple prompt and predefined configurations.
      #
      # This method wraps around the `AI.query` method to send a query to an AI client, using
      # predefined configurations such as model, temperature, and maximum tokens. The prompt
      # is packaged into a `messages` parameter, which is compatible with OpenAI's API.
      #
      # @param client [Object] The AI client instance, such as `OpenAI::Client`.
      # @param data [Hash] A hash for tracking token usage statistics. Keys include:
      #   - `:prompt_tokens` [Integer] Total tokens used in the prompt.
      #   - `:completion_tokens` [Integer] Total tokens generated by the AI.
      #   - `:cached_tokens` [Integer] Tokens retrieved from the cache, if applicable.
      # @param prompt [String] The text input from the user to be sent to the AI client.
      #
      # @return [String] The AI's response message content as a string, returned by `AI.query`.
      #
      # @example Querying with a prompt:
      #   client = OpenAI::Client.new(api_key: "your_api_key")
      #   data = { prompt_tokens: 0, completion_tokens: 0, cached_tokens: 0 }
      #   prompt = "What is the capital of France?"
      #   GPTK.query(client, data, prompt)
      #   # => "The capital of France is Paris."
      #
      # @note
      #   - This method uses configurations defined in `CONFIG` for parameters such as model, temperature, and max_tokens.
      #   - The `data` hash is updated in-place with token usage statistics, but tracking is currently marked as TODO.
      #   - This method assumes the client is compatible with OpenAI's `messages` API structure.
      #
      # @todo Implement proper token usage tracking.
      #
      # @see AI.query
      #
      def self.query(client, data, prompt)
        response = AI.query client, data, {
          model: CONFIG[:openai_gpt_model],
          temperature: CONFIG[:openai_temperature],
          max_tokens: CONFIG[:max_tokens],
          messages: [{ role: 'user', content: prompt }]
        }
        @last_output = response
        response
        # TODO: track token usage
      end

      # Creates a new assistant using the specified client and configuration parameters.
      #
      # This method interacts with an AI client to create a virtual assistant. It accepts various
      # parameters, such as name, instructions, description, tools, tool resources, and metadata,
      # and dynamically builds the necessary configuration for the request. The method sends the
      # request to the client and returns the unique identifier of the newly created assistant.
      #
      # @param client [Object] The AI client instance, such as `OpenAI::Client`.
      # @param name [String] The name of the assistant to be created.
      # @param instructions [String] Specific instructions for the assistant to guide its behavior.
      # @param description [String, nil] A brief description of the assistant's purpose (optional).
      # @param tools [Array, nil] A list of tools available to the assistant (optional).
      # @param tool_resources [Hash, nil] Resources required for the tools (optional).
      # @param metadata [Hash, nil] Additional metadata to configure the assistant (optional).
      #
      # @return [String] The unique identifier of the created assistant, as returned by the client.
      #
      # @example Creating an assistant with basic parameters:
      #   client = OpenAI::Client.new(api_key: "your_api_key")
      #   name = "ResearchBot"
      #   instructions = "Provide detailed answers for scientific queries."
      #   ChatGPT.create_assistant(client, name, instructions)
      #   # => "assistant_id_12345"
      #
      # @example Creating an assistant with advanced configurations:
      #   client = OpenAI::Client.new(api_key: "your_api_key")
      #   name = "SupportBot"
      #   instructions = "Assist users with troubleshooting steps."
      #   description = "A bot specialized in technical support."
      #   tools = ["KnowledgeBase", "LiveChat"]
      #   tool_resources = { "KnowledgeBase" => "https://example.com/api" }
      #   metadata = { "department" => "Customer Support" }
      #   ChatGPT.create_assistant(client, name, instructions, description, tools, tool_resources, metadata)
      #   # => "assistant_id_67890"
      #
      # @note
      #   - The `parameters` hash is dynamically updated to include optional keys only if their corresponding
      #     arguments are provided.
      #   - The method assumes that the client supports an `assistants.create` API with the given structure.
      #
      # @see OpenAI::Client#assistants.create
      #
      def self.create_assistant(client, name, instructions, description = nil, tools = nil, tool_resources = nil, metadata = nil)
        parameters = {
          model: CONFIG[:openai_gpt_model],
          name: name,
          description: description,
          instructions: instructions
        }
        parameters.update( tools: tools ) if tools
        parameters.update( tool_resources: tool_resources ) if tool_resources
        parameters.update( metadata: metadata ) if metadata
        response = client.assistants.create parameters: parameters
        @last_output = response
        response['id']
      end

      # Executes a thread-based assistant interaction using the given prompts.
      #
      # This method manages the interaction with an AI assistant by populating a thread
      # with user messages, initiating a run, and handling the response processing. It
      # supports both single-string and array-based prompts. The method polls the status
      # of the run, retrieves messages, and returns the final assistant response.
      #
      # @param client [Object] The AI client instance, such as `OpenAI::Client`.
      # @param thread_id [String] The unique identifier of the thread to be populated and processed.
      # @param assistant_id [String] The unique identifier of the assistant to execute the run.
      # @param prompts [String, Array<String>] The user prompts to populate the thread. Can be a single string
      #   or an array of strings.
      #
      # @return [String] The final assistant response text.
      #
      # @example Running an assistant thread with a single prompt:
      #   client = OpenAI::Client.new(api_key: "your_api_key")
      #   thread_id = "thread_123"
      #   assistant_id = "assistant_456"
      #   prompts = "What are the benefits of regular exercise?"
      #   ChatGPT.run_assistant_thread(client, thread_id, assistant_id, prompts)
      #   # => "Regular exercise improves physical health, mental well-being, and overall quality of life."
      #
      # @example Running an assistant thread with multiple prompts:
      #   client = OpenAI::Client.new(api_key: "your_api_key")
      #   thread_id = "thread_789"
      #   assistant_id = "assistant_456"
      #   prompts = ["What is the capital of France?", "Explain the theory of relativity."]
      #   ChatGPT.run_assistant_thread(client, thread_id, assistant_id, prompts)
      #   # => "The capital of France is Paris. The theory of relativity..."
      #
      # @note
      #   - The method dynamically handles single-string prompts and arrays of prompts.
      #   - Polling includes a delay (`sleep`) to prevent token throttling and race conditions.
      #   - The method handles several run statuses, including `queued`, `in_progress`, `completed`, and errors.
      #   - There is a safeguard to avoid echoed responses by retrying with a new prompt.
      #
      # @todo Implement multi-page message retrieval when `has_more` is true in the message list response.
      #
      # @raise [RuntimeError] If no prompts are provided, the method will abort execution.
      #
      # @see OpenAI::Client#messages.create
      # @see OpenAI::Client#runs.create
      # @see OpenAI::Client#runs.retrieve
      # @see OpenAI::Client#messages.list
      def self.run_assistant_thread(client, prompts, assistant_id: nil)
        raise 'Error: no prompts given!' if prompts.empty?

        # Create the Assistant if it does not exist already
        unless assistant_id
          assistant_id = if client.assistants.list['data'].empty?
                           response = client.assistants.create(
                             parameters: {
                               model: GPTK::AI::CONFIG[:openai_gpt_model],
                               name: 'AI Book generator',
                               description: 'AI Book generator',
                               instructions: @instructions
                             }
                           )
                           response['id']
                         else
                           client.assistants.list['data'].first['id']
                         end
        end

        # Create the Thread
        response = client.threads.create
        thread_id = response['id']

        # Populate the thread with messages using given prompts
        if prompts.instance_of? String
          client.messages.create thread_id: thread_id,
                                 parameters: { role: 'user', content: prompts }
        else # Array
          prompts.each do |prompt|
            client.messages.create thread_id: thread_id,
                                   parameters: { role: 'user', content: prompt }
          end
        end

        # Create a run using given thread
        response = client.runs.create thread_id: thread_id, parameters: { assistant_id: assistant_id }
        run_id = response['id']

        # Loop while awaiting status of the run
        messages = []
        loop do
          response = client.runs.retrieve id: run_id, thread_id: thread_id
          @last_output = response
          status = response['status']

          case status
          when 'queued', 'in_progress', 'cancelling'
            puts 'Processing...'
            sleep 1
          when 'completed'
            order = 'asc'
            limit = 100
            initial_response = client.messages.list(thread_id: thread_id, parameters: { order: order, limit: limit })
            messages.concat initial_response['data']
            # TODO: FINISH THIS (multi-page paging for messages)
            # if initial_response['has_more'] == true
            #   until ['has_more'] == false
            #     messages.concat client.messages.list(thread_id: thread_id, parameters: { order: order, limit: limit })
            #   end
            # end
            break
          when 'requires_action'
            puts 'Error: unhandled "Requires Action"'
          when 'cancelled', 'failed', 'expired'
            puts response['last_error'].inspect
            break
          else
            puts "Unknown status response: #{status}"
            break
          end
        end

        # Return the response text received from the Assistant after processing the run
        response = messages.last['content'].first['text']['value']
        bad_response = prompts.instance_of?(String) ? (response == prompts) : (prompts.include? response)
        while bad_response
          puts 'Error: echoed response detected from ChatGPT. Retrying...'
          sleep 10
          response = run_assistant_thread client, 'Avoid repeating the input. Turn over to Claude.'
        end
        return '' if bad_response

        sleep 1 # Important to avoid race conditions and token throttling!
        @last_output = response
        response
      end

      def self.run_batch(client, prompts)
        # Create the batch file, a temporary file named `batch.jsonl`
        puts 'Generating batch file...'
        generate_batch_file prompts

        # Upload the batch file, accounting for upload errors and confirming batch file creation
        puts 'Uploading batch file to ChatGPT...'
        batch_file = File.open 'batch.jsonl'
        timeout = 1
        begin
          batch_file.rewind if batch_file.closed? # Rewind or reopen file for upload
          response = client.files.upload parameters: { file: batch_file, purpose: 'batch' }
          batch_file_id = response['id']
          abort 'Error uploading file to ChatGPT.' unless batch_file_id
        rescue StandardError => e
          puts "Error: connection reset. Retrying with a timeout of #{timeout} seconds..."
          ap e.message
          sleep timeout # Exponential backoff could start at 1 second and double each time
          timeout *= 2
          if e.class == IOError
            batch_file = File.open 'batch.jsonl', 'r+'
          end
          retry
        end
        files = client.files.list['data']
        raise 'Error: no files found on ChatGPT!' if files.empty?

        # Submit the batch file for processing
        puts "Submitting Batch File '#{batch_file_id}' to ChatGPT..."
        response = client.batches.create(
          parameters: {
            input_file_id: batch_file_id,
            endpoint: '/v1/chat/completions',
            completion_window: '24h'
          }
        )
        batch_id = response['id']
        if batch_id
          puts "Successfully submitted Batch: '#{batch_id}'\nStatus: '#{response['status']}'"
        else abort 'Error submitting Batch File to ChatGPT.'
        end

        # Monitor the progress and status of the current batch being processed
        batch = {}
        until batch['completed_at'] do
          sleep GPTK::AI::CONFIG[:batch_ping_interval] # Wait a number of seconds before checking on the status of the batch
          batch = client.batches.retrieve id: batch_id
          unless batch['output_file_id']
            request_status = "#{batch['request_counts']['completed']} of #{batch['request_counts']['total']}"
            if batch['status'] == 'failed'
              ap batch
              raise 'Error: batch processing failed!'
            end
            puts "Batch status: '#{batch['status']}': #{request_status} requests processed..."
            next
          end
          next unless batch['error_file_id']
          error_response = client.files.content id: batch['error_file_id']
          raise "Error: #{error_response}" if error_response
          break if batch['output_file_id']
        end

        # Collect response output
        response_objects = client.files.content id: batch['output_file_id']
        raise "Error: no output found" unless response_objects
        puts "Successfully completed batch ID: '#{batch_id}'"

        # Count token usage
        $prompt_tokens = response_objects.reduce(0) do |sum, obj|
          sum + obj.dig('response', 'body', 'usage', 'prompt_tokens')
        end
        $completion_tokens = response_objects.reduce(0) do |sum, obj|
          sum + obj.dig('response', 'body', 'usage', 'completion_tokens')
        end
        $cached_tokens = response_objects.reduce(0) do |sum, obj|
          sum + obj.dig('response', 'body', 'usage', 'prompt_tokens_details', 'cached_tokens')
        end

        @last_output = response_objects
        response_objects.collect {|obj| obj.dig('response', 'body', 'choices').first['message']['content'] }
      end

      def self.initialize_assistant(client)
        response = client.assistants.list
        return response['data'].first['id'] unless response['data'].empty?

        creation_response = client.assistants.create(parameters: {
          model: GPTK::AI::CONFIG[:openai_gpt_model],
          name: 'Rails Code Generator',
          description: 'An assistant for generating and refining Rails web code.',
          instructions: 'Generate Rails ERB, SASS, and CoffeeScript files based on user input or provided file content.'
        })
        creation_response['id']
      end

      def self.send_prompt_to_thread(client, thread_id, prompt)
        client.messages.create(thread_id: thread_id, parameters: { role: 'user', content: prompt })

        # Poll for the assistant's response
        loop do
          response = client.messages.list(thread_id: thread_id, parameters: { limit: 1, order: 'desc' })
          latest_message = response['data'].first
          return latest_message['content'] if latest_message && latest_message['role'] == 'assistant'

          sleep 1
        end
      end

      private

      def self.generate_batch_file(prompts)
        File.open 'batch.jsonl', 'w' do |file|
          prompts.each_with_index do |prompt, i|
            json = {
              custom_id: "request-#{i + 1}",
              method: 'POST',
              url: '/v1/chat/completions',
              body: {
                model: GPTK::AI::CONFIG[:openai_gpt_model],
                messages: [{ role: 'user', content: prompt }],
                temperature: GPTK::AI::CONFIG[:openai_temperature],
                max_tokens: GPTK::AI::CONFIG[:max_tokens],
              }
            }.to_json
            if i == prompts.length - 1
              file.write json.chomp
            else
              file.puts json.chomp
            end
          end
          file
        end
      end
    end

    # Anthropic Claude interface
    module Claude
      # This method assumes you MUST pass either a prompt OR a messages array
      # TODO: FIX OR REMOVE THIS METHOD! CURRENTLY RETURNING 400 ERROR
      # def self.query(client, prompt: nil, messages: nil, data: nil)
      #   AI.query client, data, {
      #     model: CONFIG[:anthropic_gpt_model],
      #     max_tokens: CONFIG[:anthropic_max_tokens],
      #     messages: messages || [{ role: 'user', content: prompt }]
      #   }
      # end

      # Sends a query to the Claude API, utilizing memory for context and tracking.
      #
      # This method sends user messages to the Claude API and retrieves a response. It handles
      # string-based or array-based inputs, dynamically constructs the request body and headers,
      # and parses the response for the AI's output. If errors occur, the method retries the query.
      #
      # @param api_key [String] The API key for accessing the Claude API.
      # @param messages [String, Array<Hash>] The user input to be sent to the Claude API. If a string
      #   is provided, it is converted into an array of message hashes with `role` and `content` keys.
      #
      # @return [String] The AI's response text.
      #
      # @example Sending a single message as a string:
      #   api_key = "your_anthropic_api_key"
      #   messages = "What is the capital of Italy?"
      #   Claude.query_with_memory(api_key, messages)
      #   # => "The capital of Italy is Rome."
      #
      # @example Sending multiple messages as an array:
      #   api_key = "your_anthropic_api_key"
      #   messages = [
      #     { role: "user", content: "Tell me a joke." },
      #     { role: "user", content: "Explain quantum mechanics simply." }
      #   ]
      #   Claude.query_with_memory(api_key, messages)
      #   # => "Here’s a joke: Why did the physicist cross the road? To observe the other side!"
      #
      # @note
      #   - The method retries the query in case of errors, such as network failures, JSON parsing errors,
      #     or bad responses from the Claude API.
      #   - A delay (`sleep 1`) is included to prevent token throttling and race conditions.
      #   - The method currently lacks data tracking functionality (marked as TODO).
      #
      # @raise [JSON::ParserError] If the response body cannot be parsed as JSON.
      # @raise [RuntimeError] If no valid response is received after retries.
      #
      # @see HTTParty.post
      # @see JSON.parse
      def self.query_with_memory(api_key, messages)
        messages = messages.instance_of?(String) ? [{ role: 'user', content: messages }] : messages
        headers = {
          'x-api-key' => api_key,
          'anthropic-version' => '2023-06-01',
          'content-type' => 'application/json',
          'anthropic-beta' => 'prompt-caching-2024-07-31'
        }
        body = {
          'model': CONFIG[:anthropic_gpt_model],
          'max_tokens': CONFIG[:anthropic_max_tokens],
          'messages': messages
        }
        begin
          response = HTTParty.post(
            'https://api.anthropic.com/v1/messages',
            headers: headers,
            body: body.to_json
          )
          # TODO: track data
          # Return text content of the Claude API response
        rescue => e # We want to catch ALL errors, not just those under StandardError
          puts "Error: #{e.class}: '#{e.message}'. Retrying query..."
          sleep 10
          output = query_with_memory api_key, messages
        end
        sleep 1 # Important to avoid race conditions and especially token throttling!
        begin
          output = JSON.parse(response.body).dig 'content', 0, 'text'
        rescue JSON::ParserError => e
          puts "Error: #{e.class}. Retrying query..."
          sleep 10
          output = query_with_memory api_key, messages
        end
        if output.nil?
          ap JSON.parse response.body
          puts 'Error: Claude API provided a bad response. Retrying query...'
          sleep 10
          output = query_with_memory api_key, messages
        end
        @last_output = output
        output
      end
    end

    module Grok
      # Sends a query to the Grok API and retrieves the AI's response.
      #
      # This method constructs an HTTP request to the Grok API, sending a user prompt and optional
      # system instructions. It handles both single-string and array-based prompts, dynamically
      # builds the request payload, and parses the response for the AI's output. If errors occur,
      # the method retries the query.
      #
      # @param api_key [String] The API key for accessing the Grok API.
      # @param prompt [String, Array<String>] The user input to send to the Grok API. Can be a single string
      #   or an array of strings.
      # @param system_prompt [String, nil] Optional system-level instructions to prepend to the message array.
      #
      # @return [String] The AI's response text.
      #
      # @example Sending a single prompt to the Grok API:
      #   api_key = "your_grok_api_key"
      #   prompt = "Explain the importance of biodiversity."
      #   Grok.query(api_key, prompt)
      #   # => "Biodiversity is crucial for ecosystem resilience and human survival."
      #
      # @example Sending multiple prompts with a system instruction:
      #   api_key = "your_grok_api_key"
      #   prompt = ["What is AI?", "How does machine learning work?"]
      #   system_prompt = "You are an AI educator."
      #   Grok.query(api_key, prompt, system_prompt)
      #   # => "AI, or Artificial Intelligence, refers to the simulation of human intelligence in machines..."
      #
      # @note
      #   - The method retries queries in case of network or JSON parsing errors.
      #   - A delay (`sleep 1`) is included to prevent token throttling and race conditions.
      #   - The method supports both single-string and array-based prompts.
      #   - Currently, token usage tracking is marked as TODO.
      #
      # @raise [RuntimeError] If no valid response is received after multiple retries.
      # @raise [JSON::ParserError] If the response body cannot be parsed as JSON.
      #
      # @see HTTParty.post
      # @see JSON.parse
      #
      # @todo Look into and possibly write a fix for repeated JSON parsing errors (looping)
      def self.query(api_key, prompt, system_prompt = nil)
        headers = {
          'Authorization' => "Bearer #{api_key}",
          'content-type' => 'application/json'
        }
        messages = if prompt.instance_of?(Array)
                     prompt.collect { |p| { 'role': 'user', 'content': p } }
                   else
                     [{ 'role': 'user', 'content': prompt }]
                   end
        messages.prepend({ 'role': 'system', 'content': system_prompt }) if system_prompt
        body = {
          'model': CONFIG[:xai_gpt_model],
          'stream': false,
          'temperature': CONFIG[:xai_temperature],
          'messages': messages
        }

        max_retries = 5
        retries = 0

        begin
          response = HTTParty.post(
            'https://api.x.ai/v1/chat/completions',
            headers: headers,
            body: body.to_json
          )

          # Check if the response is nil or not successful
          if response.nil? || response.code != 200
            raise "Unexpected response: #{response.inspect}"
          end

          parsed_response = JSON.parse(response.body)
          output = parsed_response.dig('choices', 0, 'message', 'content')

          if output.nil? || output.empty?
            raise "Empty or nil output received: #{parsed_response.inspect}"
          end

          output
        rescue Net::ReadTimeout => e
          puts "Network timeout occurred: #{e.class}. Retrying query..."
          retries += 1
          if retries <= max_retries
            sleep(5)
            retry
          else
            raise "Exceeded maximum retries due to timeout errors."
          end
        rescue JSON::ParserError => e
          puts "JSON parsing error: #{e.class}. Raw response: #{response&.body.inspect}\n Retrying query..."
          retries += 1
          if retries <= max_retries
            sleep(5)
            retry
          else
            raise "Exceeded maximum retries due to JSON parsing errors."
          end
        rescue => e
          puts "Unexpected Error: #{e.class}: #{e.message}. Raw Response: #{response&.body.inspect}\nRetrying query..."
          retries += 1
          if retries <= max_retries
            sleep(5)
            retry
          else
            raise "Exceeded maximum retries due to unexpected errors."
          end
        ensure
          @last_output = output
        end
      end
    end

    # Google's Gemini
    module Gemini
      BASE_URL = 'https://generativelanguage.googleapis.com/v1beta'

      # Sends a query to the Gemini API and retrieves the AI's response.
      #
      # This method constructs an HTTP request to the Gemini API, sending a user prompt and specifying
      # the model to use. It processes the response to extract the AI's output text. The method includes
      # retry logic to handle errors such as JSON parsing issues or bad responses.
      #
      # @param api_key [String] The API key for accessing the Gemini API.
      # @param prompt [String] The user input to be sent to the Gemini API.
      # @param model [String] The AI model to use for processing the prompt. Defaults to the value of
      #   `CONFIG[:google_gpt_model]`.
      #
      # @return [String] The AI's response text.
      #
      # @example Querying the Gemini API with a prompt:
      #   api_key = "your_gemini_api_key"
      #   prompt = "What is the role of photosynthesis in plants?"
      #   Gemini.query(api_key, prompt)
      #   # => "Photosynthesis allows plants to convert light energy into chemical energy stored in glucose."
      #
      # @note
      #   - The method retries queries in case of network errors or JSON parsing failures.
      #   - A delay (`sleep 1`) is included to prevent token throttling and race conditions.
      #   - Token usage tracking is marked as TODO.
      #
      # @raise [JSON::ParserError] If the response body cannot be parsed as JSON.
      # @raise [RuntimeError] If no valid response is received after multiple retries.
      #
      # @see HTTParty.post
      # @see JSON.parse
      def self.query(api_key, prompt, model = CONFIG[:google_gpt_model])
        # Gemini manual HTTP API call
        body = { 'contents': [{ 'parts': [{ 'text': prompt }] }] }
        response = HTTParty.post(
          "#{BASE_URL}/models/#{model}:generateContent?key=#{api_key}",
          headers: { 'content-type' => 'application/json' },
          body: body.to_json
        )
        # TODO: track data
        # Return text content of the Gemini API response
        sleep 1 # Important to avoid race conditions and token throttling!
        begin
          output = JSON.parse(response.body).dig 'candidates', 0, 'content', 'parts', 0, 'text'
        rescue JSON::ParserError => e # We want to catch ALL errors, not just those under StandardError
          puts "Error: #{e.class}. Retrying query..."
          sleep 10
          output = query api_key, prompt
        end
        if output.nil?
          ap JSON.parse response.body
          puts 'Error: Gemini API provided a bad response. Retrying query...'
          sleep 10
          output = query api_key, prompt
        end
        @last_output = output
        output
      end

      # Sends a cached query to the Gemini API and retrieves the AI's response.
      #
      # TODO: RE-DOCUMENT THIS
      #
      # This method constructs an HTTP request to the Gemini API using the provided API key, body, and model.
      # It processes the response to extract the AI's output text. The method includes retry logic to handle
      # errors, such as JSON parsing failures or bad responses, and is designed for use with cached requests.
      #
      # @param api_key [String] The API key for accessing the Gemini API.
      # @param body [Hash] The request body to send to the Gemini API. This includes prompt data and other
      #   configuration options.
      # @param model [String] The AI model to use for processing the request. Defaults to the value of
      #   `CONFIG[:google_gpt_model]`.
      #
      # @return [String] The AI's response text.
      #
      # @example Sending a cached query to the Gemini API:
      #   api_key = "your_gemini_api_key"
      #   body = {
      #     'contents': [{ 'parts': [{ 'text': "What is the capital of Japan?" }] }]
      #   }
      #   Gemini.query_with_cache(api_key, body)
      #   # => "The capital of Japan is Tokyo."
      #
      # @note
      #   - The method retries queries in case of network errors or JSON parsing failures.
      #   - A delay (`sleep 1`) is included to prevent token throttling and race conditions.
      #   - The method is designed to work with cached queries and currently lacks explicit
      #     tracking functionality (marked as TODO).
      #
      # @raise [JSON::ParserError] If the response body cannot be parsed as JSON.
      # @raise [RuntimeError] If no valid response is received after multiple retries.
      #
      # @see HTTParty.post
      # @see JSON.parse
      def self.query_with_cache(api_key, body, model = CONFIG[:google_gpt_model])
        max_retries = 5
        retries = 0

        begin
          response = HTTParty.post(
            "#{BASE_URL}/models/#{model}-001:generateContent?key=#{api_key}",
            headers: { 'content-type' => 'application/json' },
            body: body.to_json
          )

          # Explicitly check the response body for nil or empty
          if response.body.nil? || response.body.empty?
            raise "Unexpected response: Body is nil or empty."
          end

          # Parse the response to extract the output
          output = JSON.parse(response.body).dig('candidates', 0, 'content', 'parts', 0, 'text')

          # Check if the output is nil or empty
          if output.nil? || output.empty?
            raise "Empty or nil output received: #{response.body.inspect}"
          end

          output
        rescue JSON::ParserError => e
          puts "JSON Parsing Error: #{e.class}: #{e.message}. Retrying..."
          retries += 1
          if retries <= max_retries
            sleep(5)
            retry
          else
            raise "Exceeded maximum retries due to JSON parsing errors."
          end
        rescue Errno::ECONNRESET, Net::ReadTimeout => e
          puts "Network Error: #{e.class}: #{e.message}. Retrying..."
          retries += 1
          if retries <= max_retries
            sleep(5)
            retry
          else
            raise "Exceeded maximum retries due to network errors."
          end
        rescue => e
          puts "Unexpected Error: #{e.class}: #{e.message}. Raw Response: #{response&.body.inspect}"
          retries += 1
          if retries <= max_retries
            sleep(5)
            retry
          else
            raise "Exceeded maximum retries due to unexpected errors."
          end
        ensure
          @last_output = output
        end
      end

      def self.query_to_rails_code(api_key, content_file: nil, prompt: nil)
        require 'httparty'
        require 'json'
        require 'awesome_print'

        raise 'Content file is required' if content_file.nil?

        min_tokens = GPTK::AI::CONFIG[:gemini_min_cache_tokens]
        model = GPTK::AI::CONFIG[:google_gpt_model]

        # Lambda to create the cache
        create_cache = lambda do |api_key, model, file_path, min_tokens|
          bytes_per_token = 4
          min_length = min_tokens * bytes_per_token

          # Use Linux CLI `base64` to encode the file
          base64_content = `base64 -i #{Shellwords.escape(file_path)}`.strip

          # Calculate and add padding
          padding_needed = [0, min_length - base64_content.bytesize].max
          padded_content = base64_content

          # Debug logging
          puts "Original file size: #{File.size(file_path)} bytes"
          puts "Base64 content size: #{base64_content.bytesize} bytes"
          puts "Padded content size: #{padded_content.bytesize} bytes"

          # Cache payload
          cache_payload = {
            model: model,
            contents: [
              {
                parts: [
                  {
                    inline_data: {
                      mime_type: 'image/jpeg',
                      data: padded_content
                    }
                  }
                ],
                role: 'user'
              }
            ],
            ttl: '300s'
          }

          # API Request
          response = HTTParty.post(
            "https://generativelanguage.googleapis.com/v1beta/cachedContents?key=#{api_key}",
            headers: { 'Content-Type' => 'application/json' },
            body: cache_payload.to_json
          )

          # Handle response
          if response.code == 200
            cache_name = JSON.parse(response.body)['name']
            puts "Cache created successfully: #{cache_name}"
            cache_name
          else
            puts "Error Response: #{response.body}"
            raise "Error creating cache: #{response.body}"
          end
        end

        # Read file content
        file_content = File.read(content_file)

        # Create cache
        cache_name = create_cache.call(api_key, model, content_file, min_tokens)

        # File types for generation
        file_types = { erb: 'ERB', sass: 'SASS', coffeescript: 'CoffeeScript' }

        # Start interactive loop if no prompt is passed
        if prompt.nil?
          puts 'No prompt provided. Enter additional instructions (type "done" to finish):'
          user_prompts = []
          loop do
            print '> '
            input = gets.strip
            break if input.downcase == 'done'

            user_prompts << input
          end
          prompt = user_prompts.join(' ')
        end

        # Query the API for each file type
        file_types.each do |file_type, label|
          puts "Generating #{label} code..."

          query_payload = {
            contents: [
              {
                parts: [
                  { text: "Generate #{label} code for a Rails app. #{prompt} ONLY return raw #{label} code, WITHOUT markdown or conversational context." }
                ],
                role: 'user'
              }
            ],
            cachedContent: cache_name
          }

          response = HTTParty.post(
            "https://generativelanguage.googleapis.com/v1beta/models/#{model}:generateContent?key=#{api_key}",
            headers: { 'Content-Type' => 'application/json' },
            body: query_payload.to_json
          )

          response_body = response.parsed_response
          ap response_body

          generated_code = response_body.dig('candidates', 0, 'content', 'parts', 0, 'text')

          if generated_code
            # Clean up any unwanted markdown formatting
            generated_code = generated_code.gsub(/```.*\n/, '').strip
          end

          if generated_code.nil? || generated_code.empty?
            puts "Error: No #{label} code generated."
          else
            file_path = GPTK::AI::CONFIG[:rails]["#{file_type}_file_path".to_sym]
            File.write(file_path, generated_code)
            puts "Successfully wrote #{label} code to #{file_path}"
          end
        end

        puts 'Rails code generation completed!'
      end
    end

    def self.last_output
      @last_output
    end

    # Executes a query against an AI client and processes the response.
    #
    # This method sends a query to the specified AI client (e.g., OpenAI's ChatGPT or Anthropic's Claude)
    # and returns the AI's response. It adjusts for differences between client APIs and handles token
    # usage tracking, response parsing, and error recovery in case of null outputs. The method also
    # includes a delay to prevent token throttling and race conditions.
    #
    # @param client [Object] The AI client instance, such as `OpenAI::Client` or Anthropic's Claude client.
    # @param data [Hash, nil] A hash for tracking token usage statistics. Keys include:
    #   - `:prompt_tokens` [Integer] Total tokens used in the prompt.
    #   - `:completion_tokens` [Integer] Total tokens generated by the AI.
    #   - `:cached_tokens` [Integer] Tokens retrieved from the cache, if applicable.
    # @param params [Hash] The query parameters to send to the AI client.
    #   - For OpenAI: Must include `parameters` key for the `chat` method.
    #   - For Anthropic: Must include `messages` key for the `create` method.
    #
    # @return [String] The AI's response message content as a string.
    #
    # @example Querying OpenAI's ChatGPT:
    #   client = OpenAI::Client.new(api_key: "your_api_key")
    #   data = { prompt_tokens: 0, completion_tokens: 0, cached_tokens: 0 }
    #   params = { model: "gpt-4", messages: [{ role: "user", content: "Hello!" }] }
    #   GPTK.query(client, data, params)
    #   # => "Hello! How can I assist you today?"
    #
    # @example Querying Anthropic's Claude:
    #   client = Anthropic::Client.new(api_key: "your_api_key")
    #   data = { prompt_tokens: 0, completion_tokens: 0, cached_tokens: 0 }
    #   params = { messages: [{ role: "user", content: "Tell me a story." }] }
    #   GPTK.query(client, data, params)
    #   # => "Once upon a time..."
    #
    # @note
    #   - The method automatically retries the query if the response is null.
    #   - A `sleep` delay of 1 second is included to prevent token throttling or race conditions.
    #   - The `data` hash is updated in-place with token usage statistics.
    #
    # @raise [RuntimeError] If no valid response is received after multiple retries.
    #
    # @see OpenAI::Client#chat
    # @see Anthropic::Client#messages
    #
    def self.query(client, data, params)
      response = if client.class == OpenAI::Client
                   client.chat parameters: params
                 else # Anthropic Claude
                   client.messages.create params
                 end
      # Count token usage
      if data
        data[:prompt_tokens] += response.dig 'usage', 'prompt_tokens'
        data[:completion_tokens] += response.dig 'usage', 'completion_tokens'
        data[:cached_tokens] += response.dig 'usage', 'prompt_tokens_details', 'cached_tokens'
      end
      sleep 1 # Important to avoid race conditions and especially token throttling!
      # Return the AI's response message (object deconstruction must be ABSOLUTELY precise!)
      output = if client.instance_of? OpenAI::Client
                 response.dig 'choices', 0, 'message', 'content'
               else # Anthropic Claude
                 response.dig 'content', 0, 'text'
               end
      if output.nil?
        puts 'Error! Null output received from ChatGPT query.'
        until output
          puts 'Retrying...'
          output = client.chat parameters: params
          sleep 10
        end
      end
      output
    end

    # Categorizes a list of items based on provided categories using an AI model.
    #
    # This method sends prompts to an AI model to categorize a given list of items into specified
    # categories. It iterates through the items, constructs categorization prompts, and collects
    # responses from the AI. The results are grouped by category and returned as a hash. Errors
    # and progress are logged throughout the process.
    #
    # @param doc [Object] An instance containing the AI client and data context for querying.
    # @param items [Array<String>] The list of items to be categorized.
    # @param categories [String] A string describing the available categories, typically enumerated.
    #
    # @return [Hash] A hash where keys are category numbers (integers) and values are arrays of items
    #   belonging to those categories.
    #
    # @example Categorizing a list of items:
    #   doc = Document.new(client: ChatGPT::Client.new, data: { prompt_tokens: 0 })
    #   items = ["Apple", "Carrot", "Chicken"]
    #   categories = "1. Fruit\n2. Vegetable\n3. Protein"
    #   AI.categorize_items(doc, items, categories)
    #   # => { 1 => ["Apple"], 2 => ["Carrot"], 3 => ["Chicken"] }
    #
    # @note
    #   - The method aborts execution if the `items` or `categories` are empty.
    #   - The AI prompt is dynamically constructed for each item using the categories.
    #   - Progress is logged to the console during the operation.
    #   - Results are cached in the `@last_output` instance variable for reuse.
    #
    # @raise [RuntimeError] If the AI fails to generate a viable response.
    # @raise [Abort] If the input items or categories are empty, or no output is generated.
    #
    # @see ChatGPT.query
    def self.categorize_items(client, items, categories)
      abort 'Error: no items found!' if items.empty?
      abort 'Error: no categories found!' if categories.empty?
      puts "Categorizing #{items.count} items..."

      # Compose the list of prompts, one per item
      prompts = items.map do |item|
        <<~PROMPT
          Based on the following categories:
    
          #{categories}
    
          Please categorize the following item:
    
          #{item}
    
          Please return JUST the category NUMBER (excluding zero), and no other text. Ensure that every item receives a valid non-zero category number.
        PROMPT
      end

      # Run the batch and get responses (one response per item/prompt)
      responses = ChatGPT.run_batch(client, prompts)

      # Build a hash: { "item_text" => category_number }
      item_category_map = {}
      responses.each_with_index do |resp, index|
        category_num = resp.to_i
        item_category_map[items[index]] = category_num
      end

      # Some basic checks
      if item_category_map.empty?
        puts 'Error: no output!'
      else
        puts "Successfully categorized #{item_category_map.size} items!"
      end

      # Cache the results for later use, if needed
      @last_output = item_category_map

      item_category_map
    end
  end
end
